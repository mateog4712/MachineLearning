{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D,Dense,Flatten, Dropout\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from scipy import ndimage, misc\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.utils import np_utils\n",
    "from numpy.random import uniform\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.core import Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers import Input\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below requires the user to set the directory to the location of the edible plants dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "C:\\Users\\Mateo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "# This section takes the pictures from the directory where it's located. It then goes into a double for loop looking at the files\n",
    "# and the folders. The image is read as an RGB image and scaled down to a 64x64 size. It and the label, which is formatted\n",
    "# through a counter, is added to a list. After the folder ends, the lists are split into training, testing, and validation.\n",
    "# These splits are appended to their corresponding lists afterwards.\n",
    "\n",
    "\n",
    "root = 'C:/Users/Mateo/Documents/Elec Engineering/Machine Learnin/edible wild plants dataset'\n",
    "counter = -1\n",
    "\n",
    "xtrain = []\n",
    "ytrain = []\n",
    "xtest = []\n",
    "ytest = []\n",
    "xvalid = []\n",
    "yvalid = []\n",
    "tempdata = []\n",
    "templabel = []\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "   \n",
    "    counter = counter +1\n",
    "    \n",
    "    Xtrain, X_test, Ytrain, y_test = train_test_split(tempdata,templabel,test_size=.1,random_state=42)\n",
    "    xtest.extend(X_test) \n",
    "    ytest.extend(y_test)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xtrain,Ytrain,test_size=.2,random_state=42)\n",
    "    xtrain.extend(X_train)\n",
    "    ytrain.extend(y_train)\n",
    "    xvalid.extend(X_test) \n",
    "    yvalid.extend(y_test)\n",
    "    \n",
    "    tempdata = []\n",
    "    templabel = []\n",
    "    for file in files:\n",
    "        \n",
    "        filepath = os.path.join(subdir, file)\n",
    "        image = ndimage.imread(filepath, mode=\"RGB\")\n",
    "        image_resized = misc.imresize(image, (64, 64))\n",
    "        tempdata.append(image_resized)\n",
    "        templabel.append(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are dimensions of the image and the number of outputs\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT\n",
    "IMAGE_CHANNEL = 3\n",
    "OUTPUTS = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint('model-Plants', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_brightness(image):\n",
    "    # Convert 2 HSV colorspace from BGR colorspace\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # Generate new random brightness\n",
    "    rand = np.random.uniform(0.3, 1.0)\n",
    "    hsv[:, :, 2] = rand*hsv[:, :, 2]\n",
    "    # Convert back to BGR colorspace\n",
    "    new_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return new_img\n",
    "\n",
    "# Zoom-in\n",
    "def zoom(image):\n",
    "    zoom_pix = np.random.randint(15, 40)\n",
    "    zoom_factor = 1 + (2*zoom_pix)/IMAGE_HEIGHT\n",
    "    image = cv2.resize(image, None, fx=zoom_factor,\n",
    "                       fy=zoom_factor, interpolation=cv2.INTER_LINEAR)\n",
    "    top_crop = (image.shape[0] - IMAGE_HEIGHT)//2\n",
    "    left_crop = (image.shape[1] - IMAGE_WIDTH)//2\n",
    "    image = image[top_crop: top_crop+IMAGE_HEIGHT,\n",
    "                  left_crop: left_crop+IMAGE_WIDTH]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the data acquired from reading in the images and applies a zoom augmentation\n",
    "length = len(xtrain)\n",
    "xtrainBright = xtrain\n",
    "ytrainBright = ytrain\n",
    "for i in range(length):\n",
    "    xy= random_brightness(xtrain[i])\n",
    "    xtrainBright.append(xy)\n",
    "    ytrainBright.append(ytrain[i])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "length = len(xvalid)\n",
    "xvalidBright = xvalid\n",
    "yvalidBright = yvalid\n",
    "for i in range(length):\n",
    "    xy= random_brightness(xvalid[i])\n",
    "    xvalidBright.append(xy)\n",
    "    yvalidBright.append(yvalid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the lists acquired from the previous line and applies image light augmentation to increase their sizes\n",
    "length = len(xtrain)\n",
    "xtrainZoom = xtrainBright\n",
    "ytrainZoom = ytrainBright\n",
    "for i in range(length):\n",
    "    xy= zoom(xtrainBright[i])\n",
    "\n",
    "    xtrainZoom.append(xy)\n",
    "    ytrainZoom.append(ytrainBright[i])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "length = len(xvalidBright)\n",
    "xvalidZoom = xvalidBright\n",
    "yvalidZoom = yvalidBright\n",
    "for i in range(length):\n",
    "    xy= zoom(xvalidBright[i])\n",
    "    xvalidZoom.append(xy)\n",
    "    yvalidZoom.append(yvalidBright[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This changes the lists that were formed prior and changes them to arrays\n",
    "Xtrain = np.array(xtrainZoom)\n",
    "Ytrain = np.array(ytrainZoom)\n",
    "Xtest = np.array(xtest)\n",
    "Ytest = np.array(ytest)\n",
    "Xvalid = np.array(xvalidZoom)\n",
    "Yvalid = np.array(yvalidZoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15692, 64, 64, 3)\n",
      "(15692,)\n",
      "(547,)\n",
      "(3936,)\n",
      "(15692, 1)\n",
      "(547, 1)\n",
      "(3936, 1)\n"
     ]
    }
   ],
   "source": [
    "# This prints the sizes of the arrays, change the labels to a vector with width 1\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)\n",
    "print(Ytest.shape)\n",
    "print(Yvalid.shape)\n",
    "\n",
    "Ytrain =np.reshape(Ytrain, (len(Ytrain),1))\n",
    "Yvalid =np.reshape(Yvalid, (len(Yvalid),1))\n",
    "Ytest =np.reshape(Ytest, (len(Ytest),1))\n",
    "print(Ytrain.shape)\n",
    "print(Ytest.shape)\n",
    "print(Yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one-hot encodes the labels to achieve equal weight changes\n",
    "train = np_utils.to_categorical(Ytrain, num_classes=62)\n",
    "Yvalid = np_utils.to_categorical(Yvalid, num_classes=62)\n",
    "Ytest = np_utils.to_categorical(Ytest, num_classes=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This normalizes the data to between 0 and 1 for the images\n",
    "Xtrain = Xtrain/255\n",
    "Xvalid = Xvalid/255\n",
    "Xtest = Xtest/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the history variable that is ouputted from model.fit and plots the validation and training accuracy\n",
    "# and loss\n",
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    fig=plt.figure(figsize=(12, 12))\n",
    "    fig.add_subplot(2, 1, 1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    fig.add_subplot(2, 1, 2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model used for traiing the data. Split up into three sections: the first two are collections of primarily normalizations and\n",
    "# convolutions with pooling and dropout afterwards; the last layer flattens it, sends it through a dense layer, applies\n",
    "# dropout and then sends it to the output layer\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv2D(32,(3,3), activation='relu', input_shape= Xtrain.shape[1:], padding = 'valid', strides=(1, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(OUTPUTS))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this compliles the model so that it can run\n",
    "\n",
    "model.compile(optimizer = optimizers.Adam(lr=0.0012, decay = .000475), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fits the training data to the model and gives a history object\n",
    "history = model.fit(Xtrain, Ytrain,batch_size=32, validation_data=(Xvalid, Yvalid), epochs=9,callbacks=callbacks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the best weights achieved in the model above\n",
    "model.load_weights('model-Plants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints out the accuracy of the model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(Xtest, Ytest, batch_size=32)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the earlier function to plot accuracy and loss\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to plot the predicted values from the model. From Keras Tutoriol for image classification\n",
    "\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  \n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "  \n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format([predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                [true_label]),\n",
    "                                color=color)\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(62), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1]) \n",
    "  predicted_label = np.argmax(predictions_array)\n",
    " \n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates predictions from the model for the test values\n",
    "\n",
    "predictions = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the predicted values next to the picture and displays if it is correct or not.\n",
    "i = 20\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions, ytest, Xtest)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions,  ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
